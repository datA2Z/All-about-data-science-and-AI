{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Part 2\n",
    "\n",
    "\n",
    "## Bigram, Clustering and Topic Modeling\n",
    "\n",
    "\n",
    "In This notebook we will performing Bigram, Clustering and Topic Modeling. if you haven't follow part one notebook you can click [here](https://nbviewer.jupyter.org/github/datA2Z/All-about-data-science-and-AI/blob/master/Natural%20language%20processing/Natural%20language%20processing%20part%201.ipynb) and follow along. lets code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the information related to this code are provided in notebook one.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.chdir(\"F:/portfolio/NLP/Sentiment analysis/Data\")\n",
    "yelp = pd.read_csv('yelp.csv')\n",
    "\n",
    "yelp['Sentiment'] = 'NA'\n",
    "for i in range(0,len(yelp)):\n",
    "    t = TextBlob(yelp['text'][i]).sentiment.polarity\n",
    "    if t < 0:\n",
    "        yelp.set_value(i,'Sentiment','Negative')\n",
    "    elif t == 0:\n",
    "        yelp.set_value(i,'Sentiment','Neutral')\n",
    "    else :\n",
    "        yelp.set_value(i,'Sentiment','Positive')\n",
    "\n",
    "\n",
    "\n",
    "pos = yelp.loc[yelp['Sentiment'] == \"Positive\"]\n",
    "neg = yelp.loc[yelp['Sentiment'] == \"Negative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram\n",
    "\n",
    "Bigrams are basically a set of co-occuring words within a given window.\n",
    "\n",
    "***Note : We are analyzing only negative text but you can perform it with other text as well!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import require library \n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text data into list\n",
    "texts = neg[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords.words('english')\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text clean funcation\n",
    "def process_text(text):\n",
    "   if text.startswith('@null'):\n",
    "       return \"[text not available]\"\n",
    "   text = re.sub(r'\\$\\w*','',text) # Remove tickers\n",
    "   text = re.sub(r'https?:\\/\\/.*\\/\\w*','',text) # Remove hyperlinks\n",
    "   text = re.sub(r'['+string.punctuation+']+', ' ',text) # Remove puncutations like 's\n",
    "   text_ok = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "   tokens = text_ok.tokenize(text)\n",
    "   tokens = [i.lower() for i in tokens if i not in stopwords and len(i) > 2 and  \n",
    "                                             i in english_vocab]\n",
    "   return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for tw in texts:\n",
    "    words += process_text(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('worst', 'ever'), ('customer', 'service'), ('parking', 'lot'), ('strip', 'mall'), ('tasted', 'like'), ('behind', 'counter'), ('mac', 'cheese'), ('even', 'though'), ('fried', 'rice'), ('gross', 'gross')]\n"
     ]
    }
   ],
   "source": [
    "# Perform bigram\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words, 5)\n",
    "finder.apply_freq_filter(5)\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, as we can see Bigrams output customer ofter complaning about ('customer', 'service'), ('parking', 'lot'), ('worst', 'ever')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Let's go further, Now we will perform clustering to understand similar texts in negative reviews. Texts can be grouped together in clusters based on closeness or ‘distance’ amongst them.\n",
    "\n",
    "Each text is pre-processed and added to a list. The list is fed to TFIDF Vectorizer to convert each text into a vector. Each value in the vector depends on how many times a word or a term appears in the text (TF) and on how rare it is amongst all text/documents (IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = []\n",
    "for tw in texts:\n",
    "    words = process_text(tw)\n",
    "    cleaned_text = \" \".join(w for w in words if len(w) > 2 and \n",
    "w.isalpha()) #Form sentences of processed words\n",
    "    cleaned_texts.append(cleaned_text)\n",
    "neg['CleanText'] = cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.77315973e-15  9.85838108e-01  9.91833374e-01 ...  9.97755512e-01\n",
      "   9.92920586e-01  9.92770666e-01]\n",
      " [ 9.85838108e-01 -1.06581410e-14  9.96470534e-01 ...  9.95751630e-01\n",
      "   9.94295377e-01  9.94358520e-01]\n",
      " [ 9.91833374e-01  9.96470534e-01 -2.22044605e-16 ...  9.92887555e-01\n",
      "   9.96491856e-01  9.94311228e-01]\n",
      " ...\n",
      " [ 9.97755512e-01  9.95751630e-01  9.92887555e-01 ...  2.10942375e-15\n",
      "   9.91922312e-01  9.97681282e-01]\n",
      " [ 9.92920586e-01  9.94295377e-01  9.96491856e-01 ...  9.91922312e-01\n",
      "  -1.11022302e-15  9.74792558e-01]\n",
      " [ 9.92770666e-01  9.94358520e-01  9.94311228e-01 ...  9.97681282e-01\n",
      "   9.74792558e-01 -4.44089210e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3))  \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)  \n",
    "feature_names = tfidf_vectorizer.get_feature_names() # num phrases  \n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "dist = 1 - cosine_similarity(tfidf_matrix)  \n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    514\n",
      "0    270\n",
      "1     18\n",
      "Name: ClusterID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 3\n",
    "km = KMeans(n_clusters=num_clusters)  \n",
    "km.fit(tfidf_matrix)  \n",
    "clusters = km.labels_.tolist()  \n",
    "neg['ClusterID'] = clusters\n",
    "print(neg['ClusterID'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows 3 clusters, with following number of text in respective clusters.\n",
    "Most of the tweets are clustered around in group Id =2. Remaining are in group id 0 and id 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : Words :\n",
      " service\n",
      " place\n",
      " like\n",
      " chicken\n",
      " good\n",
      " average\n",
      " small\n",
      " pizza\n",
      " one\n",
      " money\n",
      "Cluster 1 : Words :\n",
      " closed\n",
      " location closed\n",
      " location\n",
      " closed long\n",
      " closed long time\n",
      " closed last\n",
      " closed last month\n",
      " sad closed well\n",
      " sad closed\n",
      " closed well\n",
      "Cluster 2 : Words :\n",
      " food\n",
      " get\n",
      " place\n",
      " like\n",
      " chicken\n",
      " one\n",
      " time\n",
      " service\n",
      " back\n",
      " bad\n"
     ]
    }
   ],
   "source": [
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster {} : Words :\".format(i))\n",
    "    for ind in order_centroids[i, :10]: \n",
    "        print(' %s' % feature_names[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows words appear in each cluster\n",
    "\n",
    "### Topic Modeling\n",
    "\n",
    "\n",
    "Finding central subject in the set of documents, Text in case here. \n",
    "\n",
    "\n",
    "We are using Latent Dirichlet Allocation (LDA). LDA is commonly used to identify chosen number (say, 6) topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and functions\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcation for removing stopwords, punctuations and lemmatization \n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process texts\n",
    "texts = [text for text in cleaned_texts if len(text) > 2]\n",
    "doc_clean = [clean(doc).split() for doc in texts]\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "ldamodel = models.ldamodel.LdaModel(doc_term_matrix, num_topics=6, id2word = dictionary, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Words: \n",
      "['like', 'time', 'back', 'one', 'place', 'really']\n",
      "Topic 1: Words: \n",
      "['food', 'much', 'time', 'like', 'really', 'pedicure']\n",
      "Topic 2: Words: \n",
      "['food', 'like', 'get', 'time', 'chicken', 'tasted']\n",
      "Topic 3: Words: \n",
      "['food', 'place', 'chicken', 'like', 'one', 'get']\n",
      "Topic 4: Words: \n",
      "['one', 'place', 'like', 'food', 'would', 'get']\n",
      "Topic 5: Words: \n",
      "['place', 'time', 'food', 'like', 'get', 'one']\n"
     ]
    }
   ],
   "source": [
    "# Loop over text to find topics\n",
    "for topic in ldamodel.show_topics(num_topics=6, formatted=False, num_words=6):\n",
    "    print(\"Topic {}: Words: \".format(topic[0]))\n",
    "    topicwords = [w for (w, val) in topic[1]]\n",
    "    print(topicwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the words associated with the topics that they represent certain sentiments. Topic 0 is about place and time, Topic 1, 2, 3, 4, 5  is about food, chicken, taste etc.\n",
    "\n",
    "**That is all for this demonstration. I hope you enjoyed the notebook, and see you in the next one.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
